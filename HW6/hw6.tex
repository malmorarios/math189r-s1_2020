\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SP19}
\assignment{Homework 6}
\duedate{Monday, Apr 1, 2019}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files for problem 2 can be found under the Resource tab on course website. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.\\

\begin{problem}[1]
\textbf{(Murphy 11.2 - EM for Mixtures of Gaussians)} Show that the M step for ML
estimation of a mixture of Gaussians is given by
\begin{align*}
    \mub_k &= \frac{\sum_i r_{ik}\xx_i}{r_k}\\
    \Sigmab_k &= \frac{1}{r_k}\sum_i r_{ik}(\xx_i - \mub_k)(\xx_i - \mub_k)^\T = \frac{1}{r_k}\sum_i r_{ik}\xx_i\xx_i^\T - r_k\mub_k\mub_k^\T.
\end{align*}
\end{problem}
\begin{solution}
 From page 351 of the textbook, we see that
 	\begin{align}
		\ell(\mub_{k}, \Sigmab_{k}) &= \sum_{k} \sum_{i} r_{ik} \log p(\xx_{i} \, | \, \theta_{k}) \\
				&= -\frac{1}{2} \sum_{i} r_{ik} [ \log | \Sigmab_{k} | + (\xx_{i} - \mub_{k})^{T} \Sigmab_{k}^{-1} (\xx_{i} - \mub_{k})].
	\end{align}	
	We shall now take the respective gradients and set the equal to zero to obtain our optimality condition. Starting with $\mub_{k}$ (using product rule):
		\begin{align}
			\nabla_{\mub_{k}} \ell  &= \sum_{i} r_{ik} \Sigmab_{k}^{-1} (\xx_{i} - \mub_{k} \\
				&= \Sigmab_{k}^{-1} \sum_{i} r_{ik} (\xx_{i} - \mub_{k}) \\
				&= 0 \\
				&\implies \sum_{i} r_{ik} \xx_{i} = \mub_{k} \sum_{i} r_{ik} \implies \mub_{k} = \frac{ \sum_{i}r_{ik}\xx_{i} }{ r_{k} }.
		\end{align}
	Then for $\Sigmab_{k}$:
		\begin{align}
			\nabla_{\Sigmab_{k}} \ell &= -\frac{1}{2} \sum_{i} r_{ik} [ \Sigmab_{k}^{-1} - \Sigmab_{k}^{-1}(\xx_{i} - \mub_{k}) (\xx_{i} - \mub_{k})^{T} \Sigmab_{k}^{-1} ] = 0 \\
		\end{align}
	Cancelling a $\Sigmab_{k}^{-1}$ and the $-\frac{1}{2}$ and rearranging we have
		\begin{align}
			&\sum_{i} r_{ik} [ I - (\xx_{i} - \mub_{k}) (\xx_{i} - \mub_{k})^{T} \Sigmab_{k}^{-1} ] = 0 \\
				&\implies \sum_{i} r_{ik} = \sum_{i} r_{ik} (\xx_{i} - \mub_{k}) (\xx_{i} - \mub_{k})^{T} \Sigmab_{k}^{-1}
		\end{align}
	then right multiplying by $\Sigma_{k}$ and dividing by $r_{k}$:
		\begin{align}
			\Sigmab_{k}^{-1} = \frac{1}{r_{k}} \sum_{i} r_{ik}\xx_i\xx_i^\T - r_k\mub_k\mub_k^\T,
		\end{align}
	as desired.
\end{solution}
\newpage



\begin{problem}[2]
\textbf{(SVD Image Compression)}
In this problem, we will use the image of a scary clown online to perform image compression.  In the starter code, we have already load the image into a matrix/array for you. However, you might need internet connection to access the image and therefore successfully run the starter code. The code requires Python library Pillow in order to run.
\newline
\newline 
Plot the progression of the 100 largest singular values for the original image
and a randomly shuffled version of the same image (all on the same plot). In a single figure plot
a grid of four images: the original image, and a rank $k$ truncated SVD approximation of the original
image for $k\in\{2,10,20\}$.

\end{problem}
\begin{solution}
\vfill
\end{solution}
\newpage

\end{document}
