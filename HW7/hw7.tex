\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SP19}
\assignment{Homework 7}
\duedate{Wednesday, Apr 8, 2019}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files for problem 2 can be found under the Resource tab on course website. The plot for problem 2 generated by the sample solution has been included in the starter files for reference. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.

\begin{problem}[1]
\textbf{(Murphy 11.3 - EM for Mixtures of Bernoullis)} Show that the M step for ML estimation
of a mixture of Bernoullis is given by
\[
    \mu_{kj} = \frac{\sum_i r_{ik}x_{ij}}{\sum_i r_{ik}}.
\]
Show that the M step for MAP estimation of a mixture of Bernoullis with a $\beta(a,b)$ prior
is given by
\[
    \mu_{kj} = \frac{\left(\sum_i r_{ik}x_{ij}\right) + a - 1}{\left(\sum_i r_{ik}\right) + a + b - 2}.
\]
\end{problem}
\begin{solution}
	\begin{enumerate}
	\item 
	For a mixture of Bernoullis, the probability distribution is given by:
		\begin{equation}
			p(\xx_{i} | z_{i} = k, \thetab) = \prod_{j=1}^{D} \mu_{jk}^{x_{ij}} (1- \mu_{jk})^{1-x_{ij}},
		\end{equation}
	where we suppose above that our data consists of $D$-dimensional bit vectors. Hence, the complete data log likelihood (omitting indeces in summations) is:
		\begin{align}
			\ell(\mub) = \sum_{i} \sum_{k} r_{ik} \log p(\xx_{i} , \thetab_{k}) = \sum_{i} \sum_{k} r_{ik} \left[ \sum_{j} \xx_{ij} \log \mub_{kj} + (1-\xx_{ij}) \log(1-\mub_{kj}) \right]
		\end{align}
	which we get using the properties of the logarithm function. Taking the derivative with respect to $\mub_{kj}$ and setting it equal to zero, we have
		\begin{equation}
			\frac{ \partial \ell}{ \partial \mub_{kj} }= \sum_{i}  r_{ik} \left[ \frac{\xx_{ij}}{\mu_{kj}} - \frac{1-\xx_{ij}}{1-\mu_{kj}} \right] = 0.
		\end{equation}
	We now wish to solve for $\mu_{kj}$. Hence, the above implies
		\begin{equation}
			\frac{1}{\mu_{kj}(1-\mu_{kj})} \cdot \sum_{i}  r_{ik} \left[ \xx_{ij} - \mu_{kj} \right] = 0,
		\end{equation}
	which after rearrangement, gives us
		\begin{equation}
			\mu_{kj} = \frac{\sum_{i} r_{ik} \xx_{ij}}{\sum r_{ik}}
		\end{equation}
	the desired result.
	\item 
	The complete data log likelihood with prior is 
		\begin{align}
			\ell(\mub) = \left[ \sum_{i} \sum_{k} r_{ik} \log \pi_{ik} + \sum_{i} \sum_{k} r_{ik} \log p(\xx_{i} | \mub_{k} ) \right] + \log p( \pi ) + \sum_{k} \log p(\mub_{k})
		\end{align}
		\begin{multline}
				= \bigg[ \sum_{i} \sum_{k} r_{ik} \log \pi_{ik} + \sum_{i} \sum_{k} r_{ik} \left( \sum_{j} \xx_{ij} \log \mub_{kj} + (1-\xx_{ij})\log(1-\mub_{kj}) \right) \\ + \log p( \pi ) + (a-1) \log \mub_{kj} + (b-1) \log(1-\mub_{kj}) \bigg]
		\end{multline}.
	Taking the derivative with respect to $\mub_{kj}$ and setting that equal to zero we have:
		\begin{equation}
			\frac{ \partial \ell}{ \partial \mub_{kj} }= \sum_{i} r_{ik} \left[ \frac{\xx_{ij}}{\mub_{kj}} - \frac{1-\xx_{ij}}{1- \mub_{kj}}  + \frac{a-1}{\mub_{kj}} + \frac{b-1}{1-\mub_{kj}} \right] = 0.
		\end{equation}
	Solving for $\mub_{kj}$ we have:
		\begin{equation}
			\sum_{i} r_{ik} \left[ \frac{\xx_{ij} + a - 1}{\mub_{kj}} - \frac{1-\xx_{ij} + b - 1}{1- \mub_{kj}} \right] = \sum_{i} r_{ik} \left[ \frac{(1-\mub_{kj}) (\xx_{ij} + a - 1)}{(1-\mub_{kj})\mub_{kj}} - \frac{\mub_{kj}(1-\xx_{ij} + b - 1)}{\mub_{kj}(1- \mub_{kj})} \right]
		\end{equation}
		\begin{equation}
			= \frac{1}{\mub_{kj}(1-\mub_{kj})} \sum_{i} r_{ik} \left[ \xx_{ij} + a - 1 - \mub_{kj}\xx_{ij} - \mub_{kj}a - \mub_{kj} - \mub_{kj} + \mub_{kj}\xx_{ij} - \mub_{kj}b + \mub_{kj} \right]
		\end{equation}
	which, after rearranging, gives
		\begin{equation}
			= \frac{1}{\mub_{kj}(1-\mub_{kj})} \left[ \sum_{i} r_{ik} \xx_{ij} + a - 1 - \left( \sum_{i} r_{ik} + a + b -2 \right) \right]
		\end{equation}
	and then:
		\begin{equation}
			\mub_{kj} = \frac{ \left(\sum_{i} r_{ik} \xx_{ij}\right) + a -1}{(\sum_{i}r_{ik}) + a + b -2}
		\end{equation}
	as desired.
	\end{enumerate}
	\end{solution}
\newpage



\begin{problem}[2]
\textbf{(Lasso Feature Selection)} 
In this problem, we will use the online news popularity dataset we used in hw2pr3. In the starter code, we have already parsed the data for you. However, you might need internet connection to access the data and therefore successfully run the starter code.
\newline
\newline
First, ignoring undifferentiability at $x=0$, take $\frac{\partial |x|}{\partial x}
= \mathrm{sign} (x)$. Using this, show that $\nabla \|\xx\|_1 = \mathrm{sign}(\xx)$ where $\mathrm{sign}$ is applied
elementwise. Derive the gradient of the $\ell_1$ regularized linear regression objective
\begin{align*}
    \text{minimize: } & \|A\xx - \bb\|_2^2 + \lambda \|\xx\|_1
\end{align*}

Then, implement a gradient descent based solution of the above optimization problem for this data. Produce
the convergence plot (objective vs. iterations) for a non-trivial value of $\lambda$.
In the same figure (and different axes) produce a `regularization path' plot. Detailed
more in section 13.3.4 of Murphy, a regularization path is a plot of the optimal weight on
the $y$ axis at a given regularization strength $\lambda$ on the $x$ axis. Armed with this
plot, provide an ordered list of the top five features in predicting the log-shares of a news
article from this dataset (with justification).
\end{problem}
\begin{solution}
	Recall that $|| \xx ||_{1} = \sum_{i} | \xx_{i} |$. Hence
		\begin{equation}
			\nabla || \xx ||_{1}[i] = \frac{\partial}{\partial \xx_{i}} \sum_{i} | \xx_{i} | = \text{sign}(\xx_{i})
		\end{equation}
	(where we are choosing to abuse the computer science index notation on the i'th component of the gradient vector above for less writing. However, on closer examination, it seems we have written more in doing so than in not.) which shows that $\nabla || \xx ||_{1} = \text{sign}(\xx)$ (we have shown this component wise above). 
	
	Deriving the gradient can then be shown as follows:
		\begin{equation}
			\nabla \left[ || A\xx - \bb||_{2}^{2} + \lambda || \xx ||_{1} \right] = 2 A^{T} A \xx - 2 \bb^{T} A + \lambda \text{sign}(\xx)
		\end{equation}
	using the fact that $\nabla( \xx^{T} A^{T} A \xx) = 2A^{T} A \xx$.
	
	See github for plot.
\end{solution}
\newpage

\end{document}
